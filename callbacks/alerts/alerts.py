def trainValDiverging(h,
                      id,
                      windowsize = 10,
                      threshold = 0.1,
                      alert = lambda s: print(s)
                     ):
    """trainValDiverging(h,
                         id,
                         windowsize = 10,
                         threshold = 0.1,
                         alert = lambda s: print(s)
                        )
        
        Function that triggers an ALERT when training and cross-validation
        losses diverge more than a THRESHOLD over WINDOWSIZE epochs.

        It calculates the linear regression of both losses and compares
        the difference between the regressed values of train and validation
        losses at first and last epochs.
        Is this difference grew more than THRESHOLD, they are considered to
        be diverging, indicating overfitting and triggering ALERT.

        h -> dictionary generated by keras with training history
        
        id -> some identification of the model.
        
        windowsize -> the number of epochs to be regressed
        
        threshold -> how big of a difference between losses is to be considered
                     alarming divergence
        
        alert -> a function that receives a string with the alert message.
                 to be triggered if divergence is detected.
    """
    # if the dictionary if of the wrong format,
    # or there hasn't been enough epochs yet, do nothing
    if ('loss' not in h
        or 'val_loss' not in h
        or len(h['loss']) < windowsize
       ):
        return

    else:
        # take the relevant subset of h to apply linear regression
        window = {key: h[key][-windowsize:] for key in ['loss', 'val_loss']}

        # calculate linear regression for training and cross-validation losses
        avgT = (windowsize + 1) * windowsize / 2
        avgloss = sum(window['loss']) / len(window['loss'])
        avgvalloss = sum(window['val_loss']) / len(window['val_loss'])
        
        covTloss, covTvalloss, varT = 0, 0, 0
        for (i, loss, valloss) in zip(list(range(len(window['loss']))),
                                      window['loss'],
                                      window['val_loss']
                                     ):
            term1 = (i - avgT)
            
            covTloss += term1 * (loss - avgloss)
            covTvalloss += term1 * (valloss - avgvalloss)
            
            varT += term1 ** 2
            
        fmaker = lambda cov, var, avgX, avgY: lambda t: (t * cov/var) + ( avgY - (avgX * cov / var) )
        
        regval = lambda t: fmaker(covTloss, varT, avgT, avgloss)(t)
        regvalloss = lambda t: fmaker(covTvalloss, varT, avgT, avgvalloss)(t)

        diff = lambda t: abs(regval(t) - regvalloss(t))

        # if losses are divergent, trigger the alert
        # identifying model by id and epoch number
        if diff(windowsize - 1) - diff(0) > threshold:
            alert("ALERT ON ID {} - EPOCH {}: Training and Validation Losses diverging for the last {} epochs!!!".format(id,
                                                                len(h['loss']),
                                                                windowsize
                                                               )
                 )